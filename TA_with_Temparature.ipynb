{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers transformers accelerate bitsandbytes datasets openpyxl pandas textstat"
      ],
      "metadata": {
        "id": "dALh7pJCVZeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REHe2hQTUaed"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "from pydantic import BaseModel, Field, validator\n",
        "\n",
        "# Initialize the LLaMA model and tokenizer\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "access_token = \"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\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True, use_auth_token=access_token)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=access_token)\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Define Pydantic models for validation\n",
        "class ExcerptCode(BaseModel):\n",
        "    excerpt: str = Field(description=\"Relevant excerpt from the transcript\")\n",
        "    code: str = Field(description=\"Code generated that best represents the excerpts identified\")\n",
        "\n",
        "    @validator(\"code\")\n",
        "    def code_is_not_long(cls, value):\n",
        "        size_code = len(value.split())\n",
        "        if size_code < 2 or size_code > 5:\n",
        "            raise ValueError(f\"Each code must be between two to five words long. The code '{value}' is {size_code} words long.\")\n",
        "        return value\n",
        "\n",
        "class ExcerptCodes(BaseModel):\n",
        "    excerpt_code: List[ExcerptCode] = Field(description=\"List of excerpt and code generated\")\n",
        "\n",
        "class SubTheme(BaseModel):\n",
        "    sub_theme: str = Field(description=\"Sub-theme identified under a main theme\")\n",
        "\n",
        "class Theme(BaseModel):\n",
        "    theme: str = Field(description=\"Main theme that was identified\")\n",
        "    sub_themes: List[SubTheme] = Field(description=\"List of sub-themes under the main theme\")\n",
        "    codes: List[str] = Field(description=\"List of codes categorized by theme identified\")\n",
        "\n",
        "class Themes(BaseModel):\n",
        "    themes: List[Theme] = Field(description=\"List of themes identified\")\n",
        "\n",
        "    @validator(\"themes\")\n",
        "    def themes_is_not_long(cls, value):\n",
        "        size_list_themes = len(value)\n",
        "        if size_list_themes > 6:\n",
        "            raise ValueError(f\"The number of themes must not exceed 6. {size_list_themes} were generated.\")\n",
        "        return value\n",
        "\n",
        "def _clean_themes_data(themes):\n",
        "    unique_codes = set()\n",
        "    themes_with_unique_codes = []\n",
        "\n",
        "    for theme_data in themes[\"themes\"]:\n",
        "        theme = theme_data[\"theme\"]\n",
        "        codes = theme_data[\"codes\"]\n",
        "        cleaned_codes = list(set([code.replace(\"_\", \" \") for code in codes]))\n",
        "        unique_codes.update(cleaned_codes)\n",
        "        sub_themes = [sub_theme.sub_theme for sub_theme in theme_data.get(\"sub_themes\", [])]\n",
        "        theme_entry = {\n",
        "            \"theme\": theme,\n",
        "            \"sub_themes\": sub_themes,\n",
        "            \"codes\": list(set(cleaned_codes))\n",
        "        }\n",
        "        themes_with_unique_codes.append(theme_entry)\n",
        "    themes[\"themes\"] = themes_with_unique_codes\n",
        "    return themes\n",
        "\n",
        "def _clean_codes_data(codes):\n",
        "    return [{\"excerpt\": k, \"code\": v.replace(\"_\", \" \")} for k, v in json.loads(codes).items()]\n",
        "\n",
        "def parse_codes(_codes, _themes):\n",
        "    cleaned_data_dict = {\"Theme\": [], \"Sub-Themes\": [], \"Codes\": [], \"Excerpts from transcript\": []}\n",
        "    json1_data = json.loads(_codes)\n",
        "    json2_data = json.loads(_themes)\n",
        "\n",
        "    for theme_data in json2_data[\"themes\"]:\n",
        "        theme = theme_data[\"theme\"]\n",
        "        sub_themes = \", \".join(theme_data.get(\"sub_themes\", []))\n",
        "        codes = set(theme_data[\"codes\"])\n",
        "        code = \", \".join([d.replace(\"_\", \" \") for d in codes])\n",
        "        cleaned_data_dict[\"Theme\"].append(theme)\n",
        "        cleaned_data_dict[\"Sub-Themes\"].append(sub_themes)\n",
        "        cleaned_data_dict[\"Codes\"].append(code)\n",
        "        excerpts_combined = []\n",
        "        for c in codes:\n",
        "            if c in json1_data.keys():\n",
        "                excerpts_combined.append(json1_data[c])\n",
        "        excerpts_combined = set(excerpts_combined)\n",
        "        cleaned_data_dict[\"Excerpts from transcript\"].append(\", \".join(excerpts_combined))\n",
        "\n",
        "    return pd.DataFrame(cleaned_data_dict)\n",
        "\n",
        "def generate_text(prompt: str, max_new_tokens: int = 100, temperature: float = 0.7) -> str:\n",
        "    \"\"\"Generate text from a given prompt using the LLaMA model.\"\"\"\n",
        "    result = generator(prompt, max_new_tokens=max_new_tokens, temperature=temperature, num_return_sequences=1)\n",
        "    return result[0]['generated_text'].strip()\n",
        "\n",
        "def process_transcript(file_path: str):\n",
        "    # Read the transcript from the .txt file\n",
        "    with open(file_path, 'r') as file:\n",
        "        transcript = file.read()\n",
        "\n",
        "    # Generate summary\n",
        "    summary_prompt = f\"Summarize the following interview transcript:\\n{transcript}\\nSummary:\"\n",
        "    summary = generate_text(summary_prompt, max_new_tokens=200, temperature=0.5).strip()\n",
        "\n",
        "    # Generate codes\n",
        "    codes_prompt = f\"\"\"\n",
        "    Review the following interview transcript and identify key phrases or codes that represent significant parts of the conversation. Each code should be a short phrase or term.\n",
        "\n",
        "    Transcript:\n",
        "    {transcript}\n",
        "\n",
        "    Codes:\"\"\"\n",
        "    codes = generate_text(codes_prompt, max_new_tokens=300, temperature=0.7).strip()\n",
        "\n",
        "    # Generate themes and sub-themes\n",
        "    themes_prompt = f\"\"\"\n",
        "    Based on the following codes, identify broad themes that categorize the key points from the interview transcript. For each theme, identify relevant sub-themes.\n",
        "\n",
        "    Codes:\n",
        "    {codes}\n",
        "\n",
        "    Themes and Sub-Themes:\"\"\"\n",
        "    themes = generate_text(themes_prompt, max_new_tokens=300, temperature=0.7).strip()\n",
        "\n",
        "    # Debugging: Print outputs\n",
        "    print(\"Generated Codes:\\n\", codes)\n",
        "    print(\"Generated Themes:\\n\", themes)\n",
        "\n",
        "    # Convert the themes and codes into the required JSON format\n",
        "    try:\n",
        "        cleaned_codes = _clean_codes_data(codes)\n",
        "        cleaned_themes = _clean_themes_data(json.loads(themes))\n",
        "        return summary, json.dumps(cleaned_codes), json.dumps(cleaned_themes)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error decoding JSON:\", e)\n",
        "        print(\"Codes data:\", codes)\n",
        "        print(\"Themes data:\", themes)\n",
        "        return summary, \"Error in codes\", \"Error in themes\"\n",
        "    except Exception as e:\n",
        "        print(\"Unexpected error:\", e)\n",
        "        return summary, \"Error processing codes\", \"Error processing themes\"\n",
        "\n",
        "def save_to_excel(summary: str, codes: str, themes: str, output_file: str):\n",
        "    # Create a DataFrame with the summary, codes, and themes\n",
        "    df = pd.DataFrame({\n",
        "        \"Summary\": [summary],\n",
        "        \"Codes\": [codes],\n",
        "        \"Themes\": [themes]\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame to an Excel file\n",
        "    df.to_excel(output_file, index=False)\n",
        "\n",
        "# Example usage\n",
        "input_file = \"/content/TA_DATA.txt\"  # Replace with your input file path\n",
        "output_file = \"analysis_output.xlsx\"  # Output Excel file path\n",
        "\n",
        "# Process the transcript and save results to an Excel file\n",
        "summary, codes, themes = process_transcript(input_file)\n",
        "save_to_excel(summary, codes, themes, output_file)\n",
        "\n",
        "print(\"Analysis saved to\", output_file)\n"
      ]
    }
  ]
}